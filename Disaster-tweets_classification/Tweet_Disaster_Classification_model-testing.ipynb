{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from keras import Model\n",
    "from keras.layers import Flatten, Input, Layer, GlobalMaxPooling1D, LSTM, Bidirectional, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of characters in each tweet\n",
    "test['char_len'] = test.text.str.len()\n",
    "\n",
    "# count number of words in each tweet\n",
    "word_tokens = [len(word_tokenize(tweet)) for tweet in test.text]\n",
    "test['word_len'] = word_tokens\n",
    "\n",
    "# count number of sentence in each tweet\n",
    "sent_tokens = [len(sent_tokenize(tweet)) for tweet in test.text]\n",
    "test['sent_len'] = sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity and subjectivity\n",
    "test['polarity'] = [TextBlob(tweet).sentiment.polarity for tweet in test.text]\n",
    "test['subjectivity'] = [TextBlob(tweet).sentiment.subjectivity for tweet in test.text]\n",
    "\n",
    "#############################################################################################################################\n",
    "# exclaimation and question marks\n",
    "test['exclaimation_num'] = [tweet.count('!') for tweet in test.text]\n",
    "test['questionmark_num'] = [tweet.count('?') for tweet in test.text]\n",
    "\n",
    "#############################################################################################################################\n",
    "# count number of hashtags and mentions\n",
    "# Function for counting number of hashtags and mentions\n",
    "def count_url_hashtag_mention(text):\n",
    "    urls_num = len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
    "    word_tokens = text.split()\n",
    "    hash_num = len([word for word in word_tokens if word[0] == '#' and word.count('#') == 1]) # only appears once in front of word \n",
    "    mention_num = len([word for word in word_tokens if word[0] == '@' and word.count('@') == 1]) # only appears once in front of word \n",
    "    return urls_num, hash_num, mention_num\n",
    "\n",
    "url_num, hash_num, mention_num = zip(*[count_url_hashtag_mention(tweet) for tweet in test.text])\n",
    "test = test.assign(url_num = url_num, hash_num = hash_num, mention_num = mention_num)\n",
    "\n",
    "#############################################################################################################################\n",
    "# count number of contractions\n",
    "contractions = [\"'t\", \"'re\", \"'s\", \"'d\", \"'ll\", \"'ve\", \"'m\"]\n",
    "test['contraction_num'] = [sum([tweet.count(cont) for cont in contractions]) for tweet in test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace NaNs with 'None'\n",
    "test.keyword.fillna('None', inplace=True) \n",
    "\n",
    "#############################################################################################################################\n",
    "## Expand Contractions\n",
    "\n",
    "# Function for expanding most common contractions https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "def decontraction(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "test.text = [decontraction(tweet) for tweet in test.text]\n",
    "\n",
    "#############################################################################################################################\n",
    "## Remove Emojis\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "print(remove_emoji(\"OMG there is a volcano eruption!!! ðŸ˜­ðŸ˜±ðŸ˜·\"))\n",
    "\n",
    "test.text = test.text.apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################\n",
    "## Remove URLs\n",
    "test.text = test.text.apply(lambda x: remove_url(x))\n",
    "\n",
    "#############################################################################################################################\n",
    "## Remove Punctuations except '!?'\n",
    "\n",
    "def remove_punct(text):\n",
    "    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n",
    "    table=str.maketrans('','',new_punct)\n",
    "    return text.translate(table)\n",
    "\n",
    "test.text = test.text.apply(lambda x: remove_punct(x))\n",
    "\n",
    "#############################################################################################################################\n",
    "## Replace amp\n",
    "def replace_amp(text):\n",
    "    text = re.sub(r\" amp \", \" and \", text)\n",
    "    return text\n",
    "\n",
    "test.text = test.text.apply(lambda x: replace_amp(x))\n",
    "\n",
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordsegment import load, segment\n",
    "# load()\n",
    "\n",
    "# test.text = test.text.apply(lambda x: ' '.join(segment(x)))\n",
    "\n",
    "test = pd.read_csv('../input/twitter-logo/tweets_test_segmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(w.lower(), pos='v') for w in words])\n",
    "\n",
    "test.text = test.text.apply(lambda x: lemma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "test_text = test['text']\n",
    "test_text = tokenizer_1.texts_to_sequences(test_text)\n",
    "\n",
    "# padding\n",
    "test_text = pad_sequences(test_text, padding='post', maxlen=50)\n",
    "\n",
    "print('X_test shape:', test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm prediction\n",
    "# model.predict(test_text)\n",
    "lstm_model.load_weights('lstm_model.h5')\n",
    "submission = test.copy()[['id']]\n",
    "submission['target'] = lstm_model.predict_classes(test_text)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "display(submission.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-lstm attention prediction\n",
    "model_attn.load_weights('attn_model.h5')\n",
    "submission_attn = test.copy()[['id']]\n",
    "submission_attn['target'] = model_attn.predict(test_text)\n",
    "submission_attn['target'] = submission_attn['target'].apply(lambda x: 1 if x >=0.5 else 0)\n",
    "submission_attn.to_csv('submission_attn.csv', index=False)\n",
    "display(submission_attn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert prediction\n",
    "\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "\n",
    "bert_model.load_weights('bertmodel.h5')\n",
    "submission_bert = test.copy()[['id']]\n",
    "submission_bert['target'] = bert_model.predict(test_input)\n",
    "submission_bert['target'] = submission_bert['target'].apply(lambda x: 1 if x >=0.5 else 0)\n",
    "submission_bert.to_csv('submission_bert.csv', index=False)\n",
    "display(submission_bert.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert + meta-features prediction\n",
    "\n",
    "clf_testpred = clf.predict_proba(test.drop(['id','keyword','location','text'],axis=1))\n",
    "submission_bert = test.copy()[['id']]\n",
    "submission_bert['target'] = (bert_model.predict(test_input)*0.8).ravel() + (clf_testpred.max(axis=1)*0.2)\n",
    "submission_bert['target'] = submission_bert['target'].apply(lambda x: 1 if x >=0.5 else 0)\n",
    "submission_bert.to_csv('submission_bert_ensemble.csv', index=False)\n",
    "display(submission_bert.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_bert['target'].plot(kind='hist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
