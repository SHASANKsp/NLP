{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('Checkpoint2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Validation Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (6090, 12)\n",
      "X_val shape:  (1523, 12)\n",
      "y_train shape:  (6090, 1)\n",
      "y_val shape:  (1523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(tweets.drop(['id','keyword','location','target'],axis=1), tweets[['target']], test_size=0.2, stratify=tweets[['target']], random_state=0)\n",
    "X_train_text = X_train['text']\n",
    "X_val_text = X_val['text']\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_val shape: ', X_val.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_val shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Proportion:\n",
      " 0    57.027915\n",
      "1    42.972085\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Validation Class Proportion:\n",
      " 0    57.058437\n",
      "1    42.941563\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Train Class Proportion:\\n', y_train['target'].value_counts() / len(y_train) * 100)\n",
    "print('\\nValidation Class Proportion:\\n', y_val['target'].value_counts() / len(y_val) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Tokenization'></a>\n",
    "## 6.1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1 = Tokenizer(num_words=5000, oov_token='<UNK>')\n",
    "tokenizer_1.fit_on_texts(X_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59, 2, 4, 33, 5, 3, 1, 88, 1128, 360, 20, 998, 18, 14, 18, 15, 1423], [1, 2342, 2343, 3622, 39, 122, 117, 2, 1314, 235, 299], [3623, 339, 1, 191, 1, 1047, 80, 105], [8, 126, 9, 2, 1, 55, 1315, 881, 5, 33, 1, 435, 40, 1048, 1, 26, 12, 4, 1424, 1, 1983, 244, 10, 75, 1, 179], [9, 1, 58, 3624, 2344, 1, 499, 20, 1, 8, 272, 149, 94, 113, 8, 163, 2345, 153, 2, 716, 150], [1, 255, 7, 3, 2806], [13, 3, 164, 7, 1, 1425, 175, 2807, 11, 607, 3625, 2808], [132, 6, 561, 65, 216, 49, 81, 562, 132, 389, 21, 3, 201, 370, 7, 216, 637, 563], [134, 564, 53, 181, 10, 82, 61, 1049, 23, 350, 2809, 176, 10, 42, 118], [717, 310, 199, 23, 202, 1739, 224, 11, 1984, 7, 68, 638]]\n",
      "\n",
      "[[3, 270, 1712, 9, 97, 11, 37, 3, 209, 1341, 31, 12, 6, 177, 196, 54, 7, 17, 294, 165, 4, 1340, 4972, 4973, 622, 270, 234], [10, 2, 3, 527, 1, 377, 17, 63, 558, 353, 9, 2, 1, 1], [1, 923, 774, 626, 2137, 4013], [787, 73, 21, 861, 22, 3, 862, 816, 39, 297, 659, 741, 62, 1022, 1079], [535, 204, 39, 236, 47, 116, 233, 93, 1184, 19, 158, 473, 3, 537, 7, 3, 455, 97, 617], [10, 2, 1, 29, 930, 119, 20, 2336, 276, 6, 2492, 10, 1440, 35, 176, 16, 10, 720, 79, 2, 3863], [922, 709, 1, 7, 1, 119, 782, 667, 6, 1, 1, 19, 1, 1, 8, 1870, 50, 2, 31, 489, 11], [529, 1, 277, 990, 1, 8, 352, 334, 4721, 2908, 2023, 48], [575, 1369, 35, 791, 1, 4, 1, 667, 65, 3, 280, 399, 1244, 124, 2, 1, 30, 5, 3], [396, 1, 1197, 45, 51, 169, 51, 32, 24, 1, 71, 1, 963, 7, 1792, 8, 479, 285, 5, 59, 1882, 51, 1718, 1]]\n"
     ]
    }
   ],
   "source": [
    "X_train_text = tokenizer_1.texts_to_sequences(X_train_text)\n",
    "X_val_text = tokenizer_1.texts_to_sequences(X_val_text)\n",
    "print(X_train_text[:10])\n",
    "print('')\n",
    "print(X_val_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list in the `X_train_text` and `X_val_text` is a list of integers, which corresponds to each tweets in the train and validation set respectively. The length of each list is also different as different tweets have different lengths. Therefore, we will need to apply **padding** to make all sequences the same length.\n",
    "\n",
    "We can use `tokenizer.word_index` to look at the vocabulary dictionary and `sequences_to_texts` to transform sequences back into texts. Note that words that are not in the vocabulary are now `<UNK>`.\n",
    "\n",
    "\n",
    "**Note:** The Tokenizer stores everything in the `word_index` during `fit_on_texts`. Then, when calling the `texts_to_sequences` method, only the top `num_words` are considered. So `word_index` will actually contain more words than `num_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UNK> fest announce refund after day two be extreme weather evacuation']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_1.sequences_to_texts([X_train_text[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Padding'></a>\n",
    "## 6.2. Padding\n",
    "\n",
    "After tokenization, each tweet is represented as a list of tokens. Next, we need to **pad** all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths). Do this by adding 0s to the end of each sentence in the tokenized form so that each sentence is *now the same length as the longest tweet*. \n",
    "\n",
    "The max length for the train set tweets is 32. We will set the `maxlen` to be 50 as tweets from the validation or test set might be longer. This means that texts longer than 50 words will be truncated to the 1st 50 words while texts shorter than 50 will have 0s appended to make them of length 50.\n",
    "\n",
    "Below shows a quick example of padding sentences to a length of 5 sequences.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Max Length: 34\n",
      "X_train shape: (6090, 50)\n",
      "X_train shape: (1523, 50)\n"
     ]
    }
   ],
   "source": [
    "print('Train Set Max Length:', max(len(text) for text in X_train_text))\n",
    "maxlen = 50\n",
    "\n",
    "X_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\n",
    "X_val_text = pad_sequences(X_val_text, padding='post', maxlen=maxlen)\n",
    "\n",
    "print('X_train shape:', X_train_text.shape)\n",
    "print('X_train shape:', X_val_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='E_Matrix'></a>\n",
    "## 6.3. Embedding Matrix â€“ GloVe\n",
    "\n",
    "We will use the [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) that were pre-trained on 2 billion tweets to create our feature matrix. First, we will create a dictionary that will contain words as keys and their corresponding embedding list at values. The length of the embedding for each word will be 200, as the GloVe embedding we are using was trained to have 200 dimensions. Refer to [here](https://github.com/stanfordnlp/GloVe) also for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer_1.word_index) + 1\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.twitter.27B.200d.txt', \"r\", encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create an embedding matrix for our train vocab/corpus where each row number will correspond to the index of the word in our train vocab/corpus. The matrix will have 200 columns, each containing one of the GloVe feature/dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (12727, 200)\n"
     ]
    }
   ],
   "source": [
    "# create a weight matrix for words in training set\n",
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "\n",
    "for word, i in tokenizer_1.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Embedding Matrix Shape:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Model_Build'></a>\n",
    "# 7. Model Building & Training\n",
    "\n",
    "<a id='LSTM'></a>\n",
    "## 7.1. Long Short-Term Memory (LSTM)\n",
    "\n",
    "[Long Short-Term Memory (LSTM)](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735) models are a type of recurrent neural network that allows for longer range dependencies, unlike traditional feed-forward RNNs. It has a few advantages:\n",
    "\n",
    "1. Longer range dependence\n",
    "2. Selectively remember or forget things\n",
    "3. Get around exploding and vanishing gradients\n",
    "\n",
    "LSTMs have a few dependencies. Imagine that we are predicting whether it will rain today. This depends on several information:\n",
    "\n",
    "1. The trend of the previous few days, such as many days with rain, or a very heavy downpour (the previous cell state)\n",
    "2. Information from the previous day, such as temperature, wind level (previous hidden state)\n",
    "3. Information from today (input at current time step)\n",
    "\n",
    "To decide whether or not to use these information, LSTMs contain different memory blocks called **cells**, which are responsible for remembering which information are important to use and which to discard. Manipulations or changes to this memory is done through **gates**. \n",
    "\n",
    "1. The **forget gate** is responsible for removing memories from the cell state. From the figure and equations below, it takes inputs from the previous hidden state, `a` and current input `x`, and applies a sigmoid function to decide whether to keep the information or not. \n",
    "2. The **input gate** consists of the **update gate** and the **tanh** function. The process of adding new information to the memory cell is done through this gate. The **update gate** decides which information to be added through a sigmoid function (similar to the forget gate), and the tanh function creates the information to be added. The two outputs are then multiplied together and added to the memory cell.\n",
    "3. The **output gate** selects useful information from the current cell state and outputs it. First, it creates a vector after applying **tanh** to the memory cell, then makes a filter using sigmoid function to regulate the information that needs to be used from the previous vector, and multiply them together, thus creating the output and also the hidden state to the next cell.\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://bn1301files.storage.live.com/y4moAJV3tGM4StMGVxvRmKYHz14V8F5X2aC0T4WaJdO1M_9QAPti5-3hx69bd-KJRsASCCdYErxqDL9PeNoDkFRFCwJnzpnR3e9w24NFJoOCMj3h_7jG90QjADEDje9hXVGM4sg8ltWrcbi2vz8pCLVBYsCTAQchMBn-JRTsX5ArSXY2r8ah54G_SVTJD9oJQOA?width=1711&height=623&cropmode=none' width=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "num_epochs=15\n",
    "dropout=0.2\n",
    "recurrent_dropout=0.2\n",
    "lr=0.0005\n",
    "batch_size=128\n",
    "class_weight = {0: y_train['target'].value_counts()[1]/len(y_train), 1: y_train['target'].value_counts()[0]/len(y_train)} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use dropout and recurrent dropout to add regularization to the model, which can help with overfitting. Regular dropout works in the vertical direction of the RNN, while recurrent dropout masks the connections between the recurrent units (horizontal direction). Refer to this [post](https://stackoverflow.com/questions/44924690/keras-the-difference-between-lstm-dropout-and-lstm-recurrent-dropout) for more information.\n",
    "\n",
    "A class weight will also be used. Without it, the model makes a lot more false negatives than false positives. The weighting for the minority class (`disaster`) will be given more weighting, meaning that it will be given more contribution  to the loss computation. This is taken as `(total samples-samples of class) / total samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.layers.core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Activation, Dropout, Dense\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flatten, GlobalMaxPooling1D, LSTM\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.core'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "lstm_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)) # try adding dropout later\n",
    "lstm_model.add(LSTM(128))\n",
    "\n",
    "#model.add(Flatten())\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "adam = optimizers.Adam(lr=lr)\n",
    "lstm_model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\n",
    "print(lstm_model.summary())\n",
    "\n",
    "# best hyperparameters\n",
    "# num_epochs=15\n",
    "# dropout=0.2\n",
    "# recurrent_dropout=0.2\n",
    "# lr=0.0005\n",
    "# batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(history):   \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(range(num_epochs), history.history['acc'],'-o',\n",
    "             label='Train ACC',color='#ff7f0e')\n",
    "    plt.plot(range(num_epochs),history.history['val_acc'],'-o',\n",
    "             label='Val ACC',color='#1f77b4')\n",
    "    x = np.argmax( history.history['val_acc'] ); y = np.max( history.history['val_acc'] )\n",
    "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=200,color='#1f77b4')\n",
    "    plt.text(x-0.03*xdist,y-0.13*ydist,'max acc\\n%.2f'%y,size=14)\n",
    "    plt.ylabel('Accuracy',size=14); plt.xlabel('Epoch',size=14)\n",
    "    plt.legend(loc=(0.01,0.75))\n",
    "\n",
    "    plt2 = plt.gca().twinx()\n",
    "    plt2.plot(range(num_epochs),history.history['loss'],'-o',\n",
    "              label='Train Loss',color='#2ca02c')\n",
    "    plt2.plot(range(num_epochs),history.history['val_loss'],'-o',\n",
    "              label='Val Loss',color='#d62728')\n",
    "    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=200,color='#d62728')\n",
    "    plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
    "   # plt.ylim([-0.2, 2])\n",
    "    plt.ylabel('Loss',size=14)\n",
    "    plt.xticks(ticks=list(range(num_epochs)),labels=list(range(1, num_epochs+1)))\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(0.01, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('lstm_model.h5', monitor='val_acc', save_best_only=True)\n",
    "history = lstm_model.fit(X_train_text, y_train, batch_size=batch_size, callbacks=[checkpoint], epochs=num_epochs, \n",
    "                         class_weight=class_weight, validation_data=(X_val_text, y_val), verbose=1)\n",
    "plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Attention'></a>\n",
    "## 7.2. Bidirectional LSTM with Attention\n",
    "\n",
    "<img src = 'https://bn1301files.storage.live.com/y4mpYVhDp9W6iW73-HkwwbyvkDRtQBj8K6FIz4kb7-iQhcydjC0KzXrREYJy-Im10aox7hLJIetYLNhuusOdo6fBkgpSLnnZn2RCf2H-lfqw1CXfsXUv_wFiuf2QAK70HgeNo_Ayl3H4kIbT5FUgCLK0iS21B5uNIAgFXVKapAYwMdMYzmStGqSBkvQ_H4m_9A6?width=850&height=425&cropmode=none' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vanilla LSTM only uses information from the previous timesteps and not from the future. In many NLP problems, words that come after the current timepoint also influences the current output, although this is less likely for other applications like weather forecasting. As such, a bidirectional LSTM takes into account information from both past and future to create the output at the current timepoint, as shown by the figure above in the LSTM layer. Note that a gated recurrent unit (GRU) can also be used instead of a LSTM.\n",
    "\n",
    "Also, another limitation with encoder-decoder architectures is that the encoder has to learn to encode input sequences into a *fixed-length internal representation*, which limits the performance of these networks, especially when considering very long input sequences. This means that the encoder has to compress all the information of a source input into a fixed-length vector and pass it to the encoder. The idea of **attention** aims to search for \"a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.\" â€” [Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "As seen from the figure above, the attention layer takes the bidirection hidden layer states and multiply them to a set of attention weights, which tells how much attention the current input should be paying attention to other past and future inputs (the **context**). These outputs at each timepoint will then be concatenated, which is the context, and will be used to generate the output. There are 2 main kinds of attention: **Global** and **Local** Attention.\n",
    "* **Global Attention**: Considers all hidden states of encoder LSTM and all hidden states[(Luong et al., 2015)](https://arxiv.org/abs/1508.04025) / previous hidden states [(Bahdanau et al., 2015)](https://arxiv.org/abs/1409.0473) of the unidirectional encoder LSTM. Global attention requires lots of computation as all hidden states are considered.\n",
    "* **Local Attention**: Only a part of the encoder hidden states are considered for context vector generation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention Class\n",
    "\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention\n",
    "\n",
    "## Hyperparameters\n",
    "num_epochs=15\n",
    "dropout=0.3\n",
    "recurrent_dropout=0.3\n",
    "lr=0.0005\n",
    "batch_size=128\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, Input, Layer, GlobalMaxPooling1D, LSTM, Bidirectional, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "\n",
    "## Embedding Layer\n",
    "sequence_input = Input(shape=(maxlen,))\n",
    "embedded_sequences = Embedding(vocab_size, 200, weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "\n",
    "## RNN Layer\n",
    "lstm = Bidirectional(LSTM(128, return_sequences = True, dropout=dropout, recurrent_dropout=recurrent_dropout))(embedded_sequences)\n",
    "# Getting our LSTM outputs\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(128, return_sequences=True, return_state=True))(lstm)\n",
    "\n",
    "## Attention Layer\n",
    "att_out=attention()(lstm)\n",
    "outputs=Dense(1,activation='sigmoid')(att_out)\n",
    "model_attn = Model(sequence_input, outputs)\n",
    "\n",
    "adam = optimizers.Adam(lr=lr)\n",
    "#sgd = optimizers.sgd(lr=lr)\n",
    "model_attn.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model_attn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('attn_model.h5', monitor='val_acc', save_best_only=True)\n",
    "history_attn = model_attn.fit(X_train_text, y_train, batch_size=batch_size, callbacks=[checkpoint], epochs=num_epochs, \n",
    "                              class_weight=class_weight, validation_data=(X_val_text, y_val), verbose=1)\n",
    "plot_model_performance(history_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BERT'></a>\n",
    "## 7.3. BERT\n",
    "\n",
    "The Bidirectional Encoder Representations from Transformers (BERT) is a language model developed by Google which has achieved state-of-the-art results in a variety of NLP tasks. BERT's key innovation is applying bidirectional training (actually it is non-directional, as it reads the entire sequence of words at once) of the encoder part of a **Transformer**.\n",
    "\n",
    "To understand BERT, we need to first understand what is a Transformer. The Transformer was first introduced in the very influential paper \"Attention is All You Need\" by [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). Below is the architecture of the Transformer. It gets past the sequential nature of traditional RNNs, and instead considers all inputs at the same time using **multi-headed self-attention**. To get a better intuition of self-attention as well as a more detailed explanation of the Transformer, refer to this [post](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/). Also, since the model is no longer sequential (contains no recurrence), it uses positional encodings to \"inject some information about the relative or absolute position of the tokens in the sequence\". These positional encodings use sine and cosine functions and are added to the input embeddings at the bottom of the encoder and decoder.\n",
    "\n",
    "<img src = 'https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-19-53-10.png' align='left'>\n",
    "<img src = 'https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-20-05-30.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT model uses a multi-layer bidirectional Transformer encoder (stacks the encoder several times). Only the encoder is needed as its goal is to create a language model. It performs self-attention in both directions and is pre-trained using two unsupervised prediction tasks.\n",
    "\n",
    "**Masked Language Modelling** <br>\n",
    "15% of the words in each sequence are masked at random, and the model was trained to predict these masked words based on the context provided by the other non-masked words in the sequence. The loss function only takes into consideration the prediction of the masked values and not the non-masked words.\n",
    "\n",
    "**Next Sentence Prediction** <br>\n",
    "BERT was also pre-trained to capture the relationships between consecutive sentences. It uses pairs of sentences as its training data. 5o% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document while the other half is a random sentence from the corpus.\n",
    "\n",
    "The figure below (upper) shows how BERT takes in the input and applies masking and sentence separation. The goal of training BERT is to minimize the combined loss function of these 2 strategies. Refer to this [post](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270) for a more detailed explanation.\n",
    "\n",
    "For single sentence classification such as the current problem of classifying disaster tweet, the architecture of BERT will involve adding a classification layer (sigmoid) on top of the Transformer output for the [CLS] token (lower graph below).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src = 'https://bn1301files.storage.live.com/y4mGNBqhZEX0ARXkCSvNbAkqw5PNaxxm_STcxiBcYvZVJLhdhjaNWbmnxbhxZwxyhJzPG6B7mjWwCQEWdLKJTtM9e7Z_A2y58uKJi2HoNyaU9wB4y9L66TXdp8UVvUSNPpDJc3XBGEld4gESOXDwZRc4xSYWuG_T7a5t8lDYQ3veqOCeBgt9N3IO6tI_PxaXZJV?width=1425&height=451&cropmode=none' width=600>\n",
    "<img src = 'https://media.geeksforgeeks.org/wp-content/uploads/20200422012400/Single-Sentence-Classification-Task.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "maxlen = 160\n",
    "lr = 1e-5 # 1e-5 \n",
    "num_epochs = 3 # 5\n",
    "batch_size=16 # batch size cannot be too big for bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code for building the BERT model was taken from [Wojtek Rosa's notebook](https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data). Credit goes to him for sharing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the official tokenization script created by the Google team\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tokenization\n",
    "\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512, lr=1e-5):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(X_train.text.values, tokenizer, max_len=maxlen)\n",
    "val_input = bert_encode(X_val.text.values, tokenizer, max_len=maxlen)\n",
    "train_labels = y_train.target.values\n",
    "val_labels = y_val.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = build_model(bert_layer, max_len=maxlen, lr=lr)\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('bertmodel.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "bert_history = bert_model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_data=(val_input, val_labels),\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[checkpoint], \n",
    "    #class_weight=class_weight,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa\n",
    "\n",
    "https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb#scrollTo=c3Q9NDdmqEyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Meta-data'></a>\n",
    "## 7.4. Feature-based Model\n",
    "\n",
    "Here, we will create a feature-based model using the meta-features that we created at the beginning. The idea is to ensemble this model and the sequence models together to get better predictions. When ensembling, the outputs of this model will be given less weight compared to the neural networks as the neural networks are more likely to be better learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=15, min_samples_split=20, min_samples_leaf=2, n_jobs=-1, random_state=0)\n",
    "clf.fit(X_train.drop('text',axis=1), y_train.target.values)\n",
    "clf_pred = clf.predict_proba(X_val.drop('text',axis=1))\n",
    "\n",
    "print('Validation Accuracy:', accuracy_score(y_val.target.values, clf_pred.argmax(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pred.max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pred.max(axis=-1)*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Error'></a>\n",
    "# 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = X_val.copy()\n",
    "# val = val[['text']]\n",
    "# val['target'] = y_val\n",
    "# val['pred'] = model.predict(X_val_text)\n",
    "# val['pred'] = (val['pred']*0.8) + (clf_pred.max(axis=-1)*0.2)\n",
    "# val['pred'] = val['pred'].apply(lambda x: 1 if x >=0.5 else 0)\n",
    "# error = val[val['target'] != val['pred']]\n",
    "# error.head()\n",
    "\n",
    "bert_model.load_weights('bertmodel.h5')\n",
    "val = X_val.copy()\n",
    "val = val[['text']]\n",
    "val['target'] = y_val\n",
    "# val['pred'] = lstm_model.predict_classes(X_val_text)\n",
    "val['pred'] = bert_model.predict(val_input)\n",
    "val['pred'] = val['pred'].apply(lambda x: 1 if x >=0.5 else 0)\n",
    "error = val[val['target'] != val['pred']]\n",
    "error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm  = confusion_matrix(val.target, val.pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm,figsize=(12,8),cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Non-Disaster', 'Disaster'], fontsize=16)\n",
    "plt.yticks(range(2), ['Non-Disaster', 'Disaster'], fontsize=16)\n",
    "plt.xlabel('Predicted Label',fontsize=18)\n",
    "plt.ylabel('True Label',fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "print('Num False Negatives:',sum((val['target'] == 1) & (val['pred'] == 0)))\n",
    "print('Num False Positives:',sum((val['target'] == 0) & (val['pred'] == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be more false negatives than false positives from the validation data, meaning that more tweets are being labelled as `not disaster` when in fact they are, even after using `class_weights` to adjust for the imbalance. Perhaps `disaster` tweets can be given even more weighting depending on the goal/purpose of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in error[(error['target'] == 1) & (error['pred'] == 0)]['text'].sample(n=20, random_state=0):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Test'></a>\n",
    "# 9. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of characters in each tweet\n",
    "test['char_len'] = test.text.str.len()\n",
    "\n",
    "# count number of words in each tweet\n",
    "word_tokens = [len(word_tokenize(tweet)) for tweet in test.text]\n",
    "test['word_len'] = word_tokens\n",
    "\n",
    "# count number of sentence in each tweet\n",
    "sent_tokens = [len(sent_tokenize(tweet)) for tweet in test.text]\n",
    "test['sent_len'] = sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity and subjectivity\n",
    "test['polarity'] = [TextBlob(tweet).sentiment.polarity for tweet in test.text]\n",
    "test['subjectivity'] = [TextBlob(tweet).sentiment.subjectivity for tweet in test.text]\n",
    "\n",
    "#############################################################################################################################\n",
    "# exclaimation and question marks\n",
    "test['exclaimation_num'] = [tweet.count('!') for tweet in test.text]\n",
    "test['questionmark_num'] = [tweet.count('?') for tweet in test.text]\n",
    "\n",
    "#############################################################################################################################\n",
    "# count number of hashtags and mentions\n",
    "# Function for counting number of hashtags and mentions\n",
    "def count_url_hashtag_mention(text):\n",
    "    urls_num = len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
    "    word_tokens = text.split()\n",
    "    hash_num = len([word for word in word_tokens if word[0] == '#' and word.count('#') == 1]) # only appears once in front of word \n",
    "    mention_num = len([word for word in word_tokens if word[0] == '@' and word.count('@') == 1]) # only appears once in front of word \n",
    "    return urls_num, hash_num, mention_num\n",
    "\n",
    "url_num, hash_num, mention_num = zip(*[count_url_hashtag_mention(tweet) for tweet in test.text])\n",
    "test = test.assign(url_num = url_num, hash_num = hash_num, mention_num = mention_num)\n",
    "\n",
    "#############################################################################################################################\n",
    "# count number of contractions\n",
    "contractions = [\"'t\", \"'re\", \"'s\", \"'d\", \"'ll\", \"'ve\", \"'m\"]\n",
    "test['contraction_num'] = [sum([tweet.count(cont) for cont in contractions]) for tweet in test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace NaNs with 'None'\n",
    "test.keyword.fillna('None', inplace=True) \n",
    "\n",
    "#############################################################################################################################\n",
    "## Expand Contractions\n",
    "\n",
    "# Function for expanding most common contractions https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "def decontraction(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "test.text = [decontraction(tweet) for tweet in test.text]\n",
    "\n",
    "#############################################################################################################################\n",
    "## Remove Emojis\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "print(remove_emoji(\"OMG there is a volcano eruption!!! ðŸ˜­ðŸ˜±ðŸ˜·\"))\n",
    "\n",
    "test.text = test.text.apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################\n",
    "## Remove URLs\n",
    "test.text = test.text.apply(lambda x: remove_url(x))\n",
    "\n",
    "#############################################################################################################################\n",
    "## Remove Punctuations except '!?'\n",
    "\n",
    "def remove_punct(text):\n",
    "    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n",
    "    table=str.maketrans('','',new_punct)\n",
    "    return text.translate(table)\n",
    "\n",
    "test.text = test.text.apply(lambda x: remove_punct(x))\n",
    "\n",
    "#############################################################################################################################\n",
    "## Replace amp\n",
    "def replace_amp(text):\n",
    "    text = re.sub(r\" amp \", \" and \", text)\n",
    "    return text\n",
    "\n",
    "test.text = test.text.apply(lambda x: replace_amp(x))\n",
    "\n",
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordsegment import load, segment\n",
    "# load()\n",
    "\n",
    "# test.text = test.text.apply(lambda x: ' '.join(segment(x)))\n",
    "\n",
    "test = pd.read_csv('../input/twitter-logo/tweets_test_segmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(w.lower(), pos='v') for w in words])\n",
    "\n",
    "test.text = test.text.apply(lambda x: lemma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "test_text = test['text']\n",
    "test_text = tokenizer_1.texts_to_sequences(test_text)\n",
    "\n",
    "# padding\n",
    "test_text = pad_sequences(test_text, padding='post', maxlen=50)\n",
    "\n",
    "print('X_test shape:', test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm prediction\n",
    "# model.predict(test_text)\n",
    "lstm_model.load_weights('lstm_model.h5')\n",
    "submission = test.copy()[['id']]\n",
    "submission['target'] = lstm_model.predict_classes(test_text)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "display(submission.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-lstm attention prediction\n",
    "model_attn.load_weights('attn_model.h5')\n",
    "submission_attn = test.copy()[['id']]\n",
    "submission_attn['target'] = model_attn.predict(test_text)\n",
    "submission_attn['target'] = submission_attn['target'].apply(lambda x: 1 if x >=0.5 else 0)\n",
    "submission_attn.to_csv('submission_attn.csv', index=False)\n",
    "display(submission_attn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert prediction\n",
    "\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "\n",
    "bert_model.load_weights('bertmodel.h5')\n",
    "submission_bert = test.copy()[['id']]\n",
    "submission_bert['target'] = bert_model.predict(test_input)\n",
    "submission_bert['target'] = submission_bert['target'].apply(lambda x: 1 if x >=0.5 else 0)\n",
    "submission_bert.to_csv('submission_bert.csv', index=False)\n",
    "display(submission_bert.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert + meta-features prediction\n",
    "\n",
    "clf_testpred = clf.predict_proba(test.drop(['id','keyword','location','text'],axis=1))\n",
    "submission_bert = test.copy()[['id']]\n",
    "submission_bert['target'] = (bert_model.predict(test_input)*0.8).ravel() + (clf_testpred.max(axis=1)*0.2)\n",
    "submission_bert['target'] = submission_bert['target'].apply(lambda x: 1 if x >=0.5 else 0)\n",
    "submission_bert.to_csv('submission_bert_ensemble.csv', index=False)\n",
    "display(submission_bert.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_bert['target'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Conclusion'></a>\n",
    "# 10. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/\n",
    "\n",
    "### attention\n",
    "https://matthewmcateer.me/blog/getting-started-with-attention-for-classification/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TO DO\n",
    "* Word and Char vectorizer\n",
    "* Remove numbers? Convert numbers to words?\n",
    "* Unigrams, Bigrams and Trigrams\n",
    "* Glove; remove stopwords, clean before glove?\n",
    "* Logistic Regression, BOW, TD IDF, GloVe, BERT?\n",
    "* Check Duplicates\n",
    "* Decaying LR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
